{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e48addcf-9511-42a3-9d6b-ea73a2da15e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Dynamic Partition Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f03ab33-3d6d-40a6-9238-e9c0730c7239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Partition Overwrite\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "472e9199-d491-437d-9744-d381daed78c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "from pyspark.sql.functions import cast, to_date\n",
    "_data = [\n",
    "    [\"ORD1001\", \"P003\", 70, \"01-21-2022\"],\n",
    "    [\"ORD1004\", \"P033\", 12, \"01-24-2022\"],\n",
    "    [\"ORD1005\", \"P036\", 10, \"01-20-2022\"],\n",
    "    [\"ORD1002\", \"P016\", 2, \"01-10-2022\"],\n",
    "    [\"ORD1003\", \"P012\", 6, \"01-10-2022\"],\n",
    "]\n",
    "\n",
    "_cols = [\"order_id\", \"prod_id\", \"qty\", \"order_date\"]\n",
    "\n",
    "# Create the dataframe\n",
    "df = spark.createDataFrame(data=_data, schema=_cols)\n",
    "\n",
    "# Cast the Order date from String to Date\n",
    "df = df.withColumn(\"order_date\", to_date(\"order_date\" ,\"MM-dd-yyyy\"))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92db87f5-9a6b-46f0-8a70-25a6a7ec9568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the mode for Partition Overwrite\n",
    "spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a57d44b-e513-4c6f-8909-28ab34c49eb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets repartition the data with order_date and write\n",
    "\n",
    "df.repartition(\"order_date\") \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dataset/orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8170a83a-1ba6-4cdb-9cf7-00d11389a2cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr dataset/orders_partitioned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf515db-66c2-4de5-ab31-fbdfc8aa6ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data\n",
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "spark.read.parquet(\"dataset/orders_partitioned/\").groupBy(\"order_date\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa5d5bcc-75b7-4799-873e-eb161b10caed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets create our delta dataset for Overwrite\n",
    "\n",
    "_data = [\n",
    "    [\"ORD1010\", \"P053\", 78, \"01-24-2022\"],\n",
    "    [\"ORD1011\", \"P076\", 21, \"01-20-2022\"],\n",
    "]\n",
    "\n",
    "_cols = [\"order_id\", \"prod_id\", \"qty\", \"order_date\"]\n",
    "\n",
    "# Create the delta dataframe\n",
    "delta_df = spark.createDataFrame(data=_data, schema=_cols)\n",
    "\n",
    "# Cast the Order date from String to Date\n",
    "delta_df = delta_df.withColumn(\"order_date\", to_date(\"order_date\" ,\"MM-dd-yyyy\"))\n",
    "delta_df.printSchema()\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1895d21-e443-4cfb-8adf-25667558ee84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets write to the same location for Orders partitioned\n",
    "\n",
    "delta_df.repartition(\"order_date\") \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dataset/orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8a88d80-fecf-4806-b7bd-1ebf9f56bf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr dataset/orders_partitioned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4890aa-c1b7-4473-8bc4-25d246b0791f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data\n",
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "spark.read.parquet(\"dataset/orders_partitioned/\").groupBy(\"order_date\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09777772-0ae7-47ce-8903-8b36bef6d26a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lets follow the same example but this time with partitionOverwriteMode as \"DYNAMIC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55c23c46-4f7e-42cf-9ea8-4f1d044368fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setting the partitionOverwriteMode as DYNAMIC\n",
    "\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3c97ab1-0db6-42bd-a644-10960c2b7960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "from pyspark.sql.functions import cast, to_date\n",
    "_data = [\n",
    "    [\"ORD1001\", \"P003\", 70, \"01-21-2022\"],\n",
    "    [\"ORD1004\", \"P033\", 12, \"01-24-2022\"],\n",
    "    [\"ORD1005\", \"P036\", 10, \"01-20-2022\"],\n",
    "    [\"ORD1002\", \"P016\", 2, \"01-10-2022\"],\n",
    "    [\"ORD1003\", \"P012\", 6, \"01-10-2022\"],\n",
    "]\n",
    "\n",
    "_cols = [\"order_id\", \"prod_id\", \"qty\", \"order_date\"]\n",
    "\n",
    "# Create the dataframe\n",
    "df = spark.createDataFrame(data=_data, schema=_cols)\n",
    "\n",
    "# Cast the Order date from String to Date\n",
    "df = df.withColumn(\"order_date\", to_date(\"order_date\" ,\"MM-dd-yyyy\"))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2ff8bb1-4473-4d69-92e0-0196c4347b3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets repartition the data with order_date and write\n",
    "\n",
    "df.repartition(\"order_date\") \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dataset/orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5fc10c4-dc88-4999-9268-32fa0fe543b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr dataset/orders_partitioned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "879a4397-948c-4656-b3b1-171cf5957628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data\n",
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "spark.read.parquet(\"dataset/orders_partitioned/\").groupBy(\"order_date\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef9b828-04e0-4011-938a-346574c3ec56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets create our delta dataset for Overwrite\n",
    "\n",
    "_data = [\n",
    "    [\"ORD1010\", \"P053\", 78, \"01-24-2022\"],\n",
    "    [\"ORD1011\", \"P076\", 21, \"01-10-2022\"],\n",
    "]\n",
    "\n",
    "_cols = [\"order_id\", \"prod_id\", \"qty\", \"order_date\"]\n",
    "\n",
    "# Create the delta dataframe\n",
    "delta_df = spark.createDataFrame(data=_data, schema=_cols)\n",
    "\n",
    "# Cast the Order date from String to Date\n",
    "delta_df = delta_df.withColumn(\"order_date\", to_date(\"order_date\" ,\"MM-dd-yyyy\"))\n",
    "delta_df.printSchema()\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d8ffeb-5e3d-470b-881f-3db3ceb61971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets write to the same location for Orders partitioned\n",
    "\n",
    "delta_df.repartition(\"order_date\") \\\n",
    "    .write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .partitionBy(\"order_date\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"dataset/orders_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9904d262-eedd-4e46-8795-530538ecae29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "ls -ltr dataset/orders_partitioned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "095af19f-36ff-4d6b-9847-105ec1eb8ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate data\n",
    "from pyspark.sql.functions import count, lit\n",
    "\n",
    "spark.read.parquet(\"dataset/orders_partitioned/\").groupBy(\"order_date\").agg(count(lit(1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a3a2868-034e-4874-aee8-2c5c07ef3e03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "24_Partition_Overwrite",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
