{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73293c1c-28c8-490a-960a-9baa7981741c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Estimate Partition Count for File Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "881532fe-7cfb-40a6-9c63-9a1d2e4dc5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Estimate Partition Count\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6290ce0-e017-41a6-89b4-578746e22955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the default partition size\n",
    "partition_size = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "print(f\"Partition Size: {partition_size} in bytes and {int(partition_size) / 1024 / 1024} in MB\")\n",
    "\n",
    "# Check the default open Cost in Bytes\n",
    "open_cost_size = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "print(f\"Open Cost Size: {open_cost_size} in bytes and {int(open_cost_size) / 1024 / 1024} in MB\")\n",
    "\n",
    "# Default parallelism\n",
    "parallelism = int(spark.sparkContext.defaultParallelism)\n",
    "print(f\"Default Parallelism: {parallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "162e77f1-da8a-44de-bc48-b9be63e81eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File size in Bytes\n",
    "average_file_size = 2647733632\n",
    "total_files = 1\n",
    "\n",
    "# Total Actual File Size in Bytes\n",
    "total_file_size = average_file_size * total_files\n",
    "print(f\"Total File size on disk: {total_file_size} in bytes and {total_file_size / 1024 /1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15429690-d554-4be5-8a13-b60cef2a9d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Padded file size for Spark read\n",
    "padded_file_size = total_file_size + (total_files * open_cost_size)\n",
    "print(f\"Total padded file size: {padded_file_size} in bytes and {padded_file_size / 1024 /1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c338eae-bbaa-493d-9b9a-3792f841a977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of Bytes per Core\n",
    "bytes_per_core = padded_file_size / parallelism\n",
    "print(f\"Bytes per Core: {bytes_per_core} in bytes and {bytes_per_core / 1024 /1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6f9c25c-89b0-48bc-8f4f-a27094801556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Max Split Bytes\n",
    "max_bytes_per_split = min(partition_size, max(open_cost_size, bytes_per_core))\n",
    "print(f\"Max bytes per Partition: {max_bytes_per_split} in bytes and {max_bytes_per_split / 1024 /1024} in MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5429dbef-964d-4d2d-b226-661f2d31af0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Total number of Partitions\n",
    "num_of_partitions = padded_file_size / max_bytes_per_split\n",
    "print(f\"Approx number of partitions: {num_of_partitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa78f1ea-fc70-414b-a57b-61af49d37ad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the file to see the number of partitons\n",
    "df_1 = spark.read.format(\"csv\").option(\"header\", True).load(\"dataset/sales_combined_2.csv\")\n",
    "print(f\"Number of Partition -> {df_1.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d87479c-f362-4210-8867-7db7e5ac17bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets pack all code in single function\n",
    "def num_partitions(file_size, num_of_files, spark):\n",
    "    # Check the default partition size\n",
    "    partition_size = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "    # Check the default open Cost in Bytes\n",
    "    open_cost_size = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "    # Default parallelism\n",
    "    parallelism = int(spark.sparkContext.defaultParallelism)\n",
    "    # Total Actual File Size in Bytes\n",
    "    total_file_size = file_size * num_of_files\n",
    "    # Padded file size for Spark read\n",
    "    padded_file_size = total_file_size + (num_of_files * open_cost_size)\n",
    "    # Number of Bytes per Core\n",
    "    bytes_per_core = padded_file_size / parallelism\n",
    "    # Max Split Bytes\n",
    "    max_bytes_per_split = min(partition_size, max(open_cost_size, bytes_per_core))\n",
    "    # Total number of Partitions\n",
    "    num_of_partitions = padded_file_size / max_bytes_per_split\n",
    "    \n",
    "    return num_of_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2635852a-af32-42db-8f65-3d24faab2b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validation 1\n",
    "# Calculate the number of partitions as per our logic\n",
    "estimated_num_partition = num_partitions(2647733632, 1, spark)\n",
    "print(f\"Estimated number of partitions = {estimated_num_partition}\")\n",
    "\n",
    "# Lets read a single csv file with approx size 2647733632 bytes or 2.5 GB\n",
    "df_1 = spark.read.format(\"csv\").option(\"header\", True).load(\"dataset/sales_combined_2.csv\")\n",
    "print(f\"Number of Partition -> {df_1.rdd.getNumPartitions()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb458b85-9735-40aa-921f-a1b7b87cee26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validation 2\n",
    "# Calculate the number of partitions as per our logic for tiny files\n",
    "estimated_num_partition = num_partitions(7777, 41300, spark)\n",
    "print(f\"Estimated number of partitions = {estimated_num_partition}\")\n",
    "\n",
    "# Lets read multiple parquet file with approx size 7777 bytes or 7.7 KB\n",
    "df_2 = spark.read.format(\"parquet\").load(\"dataset/sales_trx_id.parquet/\")\n",
    "print(f\"Number of Partition -> {df_2.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c6f76d7-10e8-4d10-bd78-f3b31abaeb75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validation 3\n",
    "# Calculate the number of partitions as per our logic for avg files\n",
    "estimated_num_partition = num_partitions(1159176, 220, spark)\n",
    "print(f\"Estimated number of partitions = {estimated_num_partition}\")\n",
    "\n",
    "# Lets read multiple parquet file with approx size 1159176 bytes or 1.1 MB\n",
    "df_3 = spark.read.format(\"parquet\").load(\"dataset/sales_city_id.parquet/\")\n",
    "print(f\"Number of Partition -> {df_3.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c641808-6626-45c1-ad76-5407419f3f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6515a3-f530-4567-abfc-58f69ff1fb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "39_estimate_partition_count_file_read",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
