{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a505679-e5e1-4019-a323-228660b94380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read JSON Data from a Dataframe Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2e281ad-77ad-4977-a159-9e2e90e276ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Read JSON data\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54dd1e9b-f8d3-47e7-8314-53e7c43b8652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example Data Frame with column having JSON data\n",
    "_data = [\n",
    "    ['EMP001', '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}'],\n",
    "    ['EMP002', '{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}'],\n",
    "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"]}']\n",
    "]\n",
    "\n",
    "# Columns for the data\n",
    "_cols = ['emp_no', 'raw_data']\n",
    "\n",
    "# Lets create the raw Data Frame\n",
    "df_raw = spark.createDataFrame(data = _data, schema = _cols)\n",
    "df_raw.printSchema()\n",
    "df_raw.show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db1e6899-e322-40fd-ba38-bd31d950d2bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to parse the JSON values from the Data Frame column - raw_data\n",
    "\n",
    "# Determine the schema of the JSON payload from the column\n",
    "json_schema_df = spark.read.json(df_raw.rdd.map(lambda row: row.raw_data))\n",
    "json_schema = json_schema_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4f6f1d1-ca98-495f-b0dc-9b244f05d937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the schema to payload to read the data\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df_details = df_raw.withColumn(\"parsed_data\", from_json(df_raw[\"raw_data\"], json_schema)).drop(\"raw_data\")\n",
    "df_details.printSchema()                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199a7789-a862-42a9-a29a-ee273cce7502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets verify the data\n",
    "df_details.select(\"emp_no\", \"parsed_data.*\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95a9435d-d112-4fbc-be2e-f8e430cd1c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We can explode the data further from list\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_details.select(\"emp_no\", \"parsed_data.dept\", \"parsed_data.fname\", \"parsed_data.lname\", \"parsed_data\") \\\n",
    "    .withColumn(\"skills\", explode(\"parsed_data.skills\")) \\\n",
    "    .drop(\"parsed_data\") \\\n",
    "    .show(100, False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_read_json_from_df_column",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
