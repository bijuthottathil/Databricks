{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bde1397-bb79-425a-8050-9333d55369c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from requests.exceptions import ConnectionError, Timeout\n",
    "import time\n",
    "\n",
    "url = \"https://health.data.ny.gov/resource/xdss-u53e.json\"\n",
    "params = {\"test_date\": \"2023-08-30T00:00:00.000\"}\n",
    "\n",
    "# Function to make the request with retries\n",
    "def get_data_with_retries(url, params, retries=3, delay=5):\n",
    "    for i in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            return response.json()\n",
    "        except (ConnectionError, Timeout) as e:\n",
    "            print(f\"Attempt {i+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    raise Exception(\"Failed to retrieve data after multiple attempts\")\n",
    "\n",
    "data = get_data_with_retries(url, params)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea9cfc9-4ec3-4bb3-be9b-53356e1c073e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Define the JSON payload schema emitted by Debezium:\n",
    "value_schema = StructType([\n",
    "  StructField(\"after\", StructType([ \n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"first_name\", StringType()),\n",
    "      StructField(\"last_name\", StringType()),\n",
    "      StructField(\"email\", StringType())\n",
    "  ])),\n",
    "  StructField(\"op\", StringType()),           # c = create, u = update, d = delete\n",
    "  StructField(\"ts_ms\", IntegerType())        # event timestamp\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b4ba2a-21bd-4e0f-9224-2f82272d2c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Stream from the Kafka topic:\n",
    "df_raw = (\n",
    "  spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"dbserver1.inventory.customers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd2fc8d-c8c0-48df-89a0-0c66f5d93fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Parse the JSON payload:\n",
    "df_parsed = (\n",
    "  df_raw\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "    .select(from_json(col(\"json_str\"), value_schema).alias(\"data\"))\n",
    "    .select(\n",
    "      col(\"data.after.id\").alias(\"id\"),\n",
    "      col(\"data.after.first_name\").alias(\"first_name\"),\n",
    "      col(\"data.after.last_name\").alias(\"last_name\"),\n",
    "      col(\"data.after.email\").alias(\"email\"),\n",
    "      col(\"data.op\").alias(\"operation\"),\n",
    "      col(\"data.ts_ms\").alias(\"event_ts\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ede70e-44f8-4b07-96e2-8d376deadd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Write into a Delta table in append mode:\n",
    "\n",
    "checkpoint = \"s3://databricksbijubucketnew/checkpoints/\"\n",
    "query = (\n",
    "  df_parsed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint}/kafka\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(\"bijucatalog.bijubronzeschema.customers_changes\")\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a6ca0e-c1e5-4d3c-913d-6e5ea10e1f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bijucatalog.bijubronzeschema.customers_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a9fd6ba-db7b-4f1b-89a0-ca82bda59240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fc6e158-e104-45ae-9db7-bf790faf7ed5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 1. Define the JSON payload schema emitted by Debezium:\n",
    "value_schema = StructType([\n",
    "  StructField(\"after\", StructType([ \n",
    "      StructField(\"id\", IntegerType()),\n",
    "      StructField(\"first_name\", StringType()),\n",
    "      StructField(\"last_name\", StringType()),\n",
    "      StructField(\"email\", StringType())\n",
    "  ])),\n",
    "  StructField(\"op\", StringType()),           # c = create, u = update, d = delete\n",
    "  StructField(\"ts_ms\", IntegerType())        # event timestamp\n",
    "])\n",
    "\n",
    "# 2. Stream from the Kafka topic:\n",
    "df_raw = (\n",
    "  spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"Ydocker.for.mac.host.internal:9092\")\n",
    "    .option(\"subscribe\", \"dbserver1.inventory.customers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# 3. Parse the JSON payload:\n",
    "df_parsed = (\n",
    "  df_raw\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "    .select(from_json(col(\"json_str\"), value_schema).alias(\"data\"))\n",
    "    .select(\n",
    "      col(\"data.after.id\").alias(\"id\"),\n",
    "      col(\"data.after.first_name\").alias(\"first_name\"),\n",
    "      col(\"data.after.last_name\").alias(\"last_name\"),\n",
    "      col(\"data.after.email\").alias(\"email\"),\n",
    "      col(\"data.op\").alias(\"operation\"),\n",
    "      col(\"data.ts_ms\").alias(\"event_ts\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Write into a Delta table in append mode:\n",
    "query = (\n",
    "  df_parsed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/debezium_to_delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .table(\"inventory.customers_changes\")\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7415312290256135,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "kafka-cdc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
