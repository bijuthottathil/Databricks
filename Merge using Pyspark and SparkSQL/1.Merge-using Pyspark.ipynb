{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52927dc-697f-4f65-86ea-4a7c8c446532",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType# Define schema for employee data\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5db89e8-e270-4a91-9526-d37af806fcf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initial employee data (DataFrame 1)\n",
    "initial_employee_data = [\n",
    "    (101, \"Alice\", \"Smith\", \"alice.s@example.com\", \"Sales\", \"2020-01-15\"),\n",
    "    (102, \"Bob\", \"Johnson\", \"bob.j@example.com\", \"Marketing\", \"2019-03-22\"),\n",
    "    (103, \"Charlie\", \"Brown\", \"charlie.b@example.com\", \"Engineering\", \"2021-06-01\"),\n",
    "    (104, \"Diana\", \"Prince\", \"diana.p@example.com\", \"HR\", \"2018-11-10\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31527fae-4626-491a-878a-39d64e3054cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, current_timestamp\n",
    "# Convert to DataFrame\n",
    "df_initial_employees = spark.createDataFrame(initial_employee_data, schema=employee_schema) \\\n",
    "                            .withColumn(\"created_at\", current_timestamp()) \\\n",
    "                            .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "print(\"Initial Employee Data:\")\n",
    "df_initial_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96294182-da9c-48d6-8333-176ffd0ead88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the initial DataFrame to a Delta table\n",
    "# We are creating an external table using 'path' option. If you want a managed table,\n",
    "# remove .option(\"path\", ...) and use .saveAsTable(\"catalog.schema.table_name\")\n",
    "spark.sql(\"use general_catalog.silver_schema\")\n",
    "df_initial_employees.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .saveAsTable(\"silver_employee\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "049beb94-59a8-48ef-929f-c73ce58513fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from silver_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2202cce6-06bb-47ed-bf2e-cb0ad1326306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC ## 3. Simulate Incremental Data - First Batch (DataFrame 2)\n",
    "# MAGIC\n",
    "# MAGIC This batch includes:\n",
    "# MAGIC - A new employee (ID 105).\n",
    "# MAGIC - An update to an existing employee (ID 101 - department change).\n",
    "# MAGIC - An update to an existing employee (ID 102 - email change).\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Incremental employee data (DataFrame 2)\n",
    "\n",
    "from pyspark.sql.functions import to_date, current_timestamp\n",
    "spark.sql(\"use general_catalog.silver_schema\")\n",
    "incremental_employee_data_1 = [\n",
    "    (101, \"Alice\", \"Smith\", \"alice.smith.new@example.com\", \"Marketing\", \"2020-01-15\"), # Update existing (email and department)\n",
    "    (102, \"Bob\", \"Johnson\", \"bobjohnson@example.com\", \"Marketing\", \"2019-03-22\"),   # Update existing (email)\n",
    "    (105, \"Emily\", \"Clark\", \"emily.c@example.com\", \"Finance\", \"2022-09-01\")       # New employee\n",
    "]\n",
    "\n",
    "df_incrimental_employee_data_1=spark.createDataFrame(incremental_employee_data_1,schema=employee_schema) \\\n",
    "    .withColumn(\"created_at\", current_timestamp()) \\\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n",
    "\n",
    "df_incrimental_employee_data_1.show()\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065bbc7d-1fbe-421f-823a-9ad7472172a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the DeltaTable object for the target table\n",
    "from delta.tables import DeltaTable\n",
    "print(\"Performing first MERGE operation...\")\n",
    "delta_table = DeltaTable.forName(spark, \"general_catalog.silver_schema.silver_employee\")\n",
    "# Perform the merge operation\n",
    "delta_table.alias(\"target\") \\\n",
    "  .merge(\n",
    "    df_incrimental_employee_data_1.alias(\"source\"),\n",
    "    \"target.employee_id = source.employee_id\" # Match condition\n",
    "  ) \\\n",
    "  .whenMatchedUpdate(set = { # What to do when a match is found (update existing record)\n",
    "    \"first_name\": \"source.first_name\",\n",
    "    \"last_name\": \"source.last_name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"department\": \"source.department\",\n",
    "    \"hire_date\": \"source.hire_date\",\n",
    "    \"updated_at\": \"source.updated_at\"\n",
    "  }) \\\n",
    "  .whenNotMatchedInsert(values = { # What to do when no match is found (insert new record)\n",
    "    \"employee_id\": \"source.employee_id\",\n",
    "    \"first_name\": \"source.first_name\",\n",
    "    \"last_name\": \"source.last_name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"department\": \"source.department\",\n",
    "    \"hire_date\": \"source.hire_date\",\n",
    "    \"created_at\": \"source.updated_at\", # For new inserts, created_at is the current updated_at\n",
    "    \"updated_at\": \"source.updated_at\"\n",
    "  }) \\\n",
    "  .execute()\n",
    "\n",
    "print(\"First MERGE operation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ecbdc1-90bf-411a-a0f2-0ef4dd9fe346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from general_catalog.silver_schema.silver_employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f254e21c-413e-4dd1-abde-40e99fcf20c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6539338708854508,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1.Merge-using Pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
