{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50b2ab2b-d640-4d88-9955-e2fd6947e162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, sum, col, from_json, current_timestamp, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark Session (in Databricks notebooks, this is pre-initialized)\n",
    "spark = SparkSession.builder.appName(\"WatermarkExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14d668b-81e1-4f38-a198-d7ff6657c954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b6f918-d931-4cba-8114-913415bde169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "class MyStreamingListener(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        print(f\"Query started: {event.id}, Name: {event.name}\")\n",
    "    def onQueryProgress(self, event):\n",
    "        print(f\"Query progress: {event.progress}\")\n",
    "    def onQueryTerminated(self, event):\n",
    "        print(f\"Query terminated: {event.id}\")\n",
    "\n",
    "spark.streams.addListener(MyStreamingListener())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e886f6f9-f3c7-4cc4-aa72-a6b55c559156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_source AS \n",
    "SELECT CAST(NULL AS STRING) AS item_id, \n",
    "       CAST(NULL AS DOUBLE) AS amount, \n",
    "       CAST(NULL AS TIMESTAMP) AS event_time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460f4a6f-76fd-424a-9c41-e2c21be5663d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from sales_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc2e238-9722-418b-aa41-028edfe82d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def inject_data(item_id, amount, delay_seconds=0):\n",
    "    current_ts = current_timestamp().cast(TimestampType())\n",
    "    if delay_seconds > 0:\n",
    "        # Simulate late arrival by subtracting delay from current_timestamp\n",
    "        current_ts = (current_timestamp() - expr(f\"INTERVAL {delay_seconds} SECONDS\")).cast(TimestampType())\n",
    "    \n",
    "    data = [(item_id, amount, current_ts)]\n",
    "    df_to_inject = spark.createDataFrame(data, sales_schema)\n",
    "    df_to_inject.write.format(\"delta\").mode(\"append\").saveAsTable(\"bijucatalog.bijusilverschema.sales_stream_input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a498c6c-aa59-44f5-88bc-1812bbd0c291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df = spark.readStream.format(\"delta\").table(\"bijucatalog.bijusilverschema.sales_stream_input\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a6519d-ed6b-4a2b-80b7-0c8660da6686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowed_sales = sales_df \\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 hour\", \"30 minutes\"),\n",
    "        col(\"item_id\")\n",
    "    ) \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "    .select(\n",
    "        col(\"item_id\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"total_sales\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61cb3e3a-6173-4f59-ae55-0595c1edf8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"s3://databricksbijubucketnew/checkpoints/\"\n",
    "query = windowed_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint}/windowed_sales\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .toTable(\"bijucatalog.bijusilverschema.hourly_item_sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79997ef8-d131-49dd-887d-c2813885bad6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import current_timestamp, expr\n",
    "def inject_data(item_id, amount, delay_seconds=0):\n",
    "    current_ts = datetime.now() - timedelta(seconds=delay_seconds)\n",
    "    data = [(item_id, amount, current_ts)]\n",
    "    df_to_inject = spark.createDataFrame(data, sales_schema)\n",
    "    df_to_inject.write.format(\"delta\").mode(\"append\").saveAsTable(\"bijucatalog.bijusilverschema.sales_stream_input\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bc231b-41f5-4a82-bf6f-5be35a8e5a9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inject_data(\"laptop\", 1200.0)\n",
    "time.sleep(5)\n",
    "inject_data(\"keyboard\", 75.0)\n",
    "time.sleep(30)\n",
    "# inject_data(\"laptop\", 50.0, delay_seconds=15 * 60)  # 15 minutes late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc34e6d-22ba-41fb-b46f-e3f65f1b141c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bijucatalog.bijusilverschema.sales_stream_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d660e859-9f6d-411f-9c5a-304fb40b0041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM bijucatalog.bijusilverschema.hourly_item_sales ORDER BY window_start DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6cda915-3e9f-4cfc-bd46-71e48ee5532e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d6aee9-86cc-4fd6-9675-e42bd8690326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from sales_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e601b410-3f6f-434b-b536-de948a5a62fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading from the simulated Delta table as a stream\n",
    "sales_df = spark.readStream.format(\"delta\").table(\"bijucatalog.bijusilverschema.sales_stream_input\")\n",
    "checkpoint = \"s3://databricksbijubucketnew/checkpoints/\"\n",
    "# Apply watermark and perform windowed aggregation\n",
    "windowed_sales = sales_df \\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 hour\", \"30 minutes\"), # 1-hour window, sliding every 30 minutes\n",
    "        col(\"item_id\")\n",
    "    ) \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "    .select(\n",
    "        col(\"item_id\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"total_sales\")\n",
    "    )\n",
    "\n",
    "# Write the results to a Delta table in \"append\" mode\n",
    "# For windowed aggregations, \"append\" mode is common, meaning results are emitted once the window is closed by the watermark.\n",
    "query = windowed_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint}/windowed_sales\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .toTable(\"hourly_item_sales\")\n",
    "\n",
    "# Start the query (in Databricks, this runs in the background)\n",
    "# query.start() \n",
    "\n",
    "# You can manually inject data to see the effect\n",
    "# inject_data(\"laptop\", 1200.0)\n",
    "# time.sleep(5)\n",
    "# inject_data(\"keyboard\", 75.0)\n",
    "# time.sleep(30) # Wait for watermark to potentially advance\n",
    "# inject_data(\"laptop\", 50.0, delay_seconds=15 * 60) # Late data for a past window\n",
    "\n",
    "# To stop the query\n",
    "# query.stop()\n",
    "\n",
    "# To view the results (after the stream has processed some data)\n",
    "# display(spark.sql(\"SELECT * FROM hourly_item_sales ORDER BY window_start DESC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733cb3cf-23a7-44cd-9432-d14a916e9c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, sum, col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark Session (already done in Databricks notebooks)\n",
    "spark = SparkSession.builder.appName(\"WatermarkExample\").getOrCreate()\n",
    "checkpoint = \"s3://databricksbijubucketnew/checkpoints/\"\n",
    "# Define the schema for incoming sales data\n",
    "sales_schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Simulate a streaming source (e.g., Kafka, Delta Lake)\n",
    "# For demonstration, we'll use a memory stream or read from a simulated file source\n",
    "# In a real scenario, this would be:\n",
    "# sales_df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"host:port\").option(\"subscribe\", \"sales_topic\").load()\n",
    "# or\n",
    "# sales_df = spark.readStream.format(\"delta\").load(\"/path/to/delta/sales_table\")\n",
    "\n",
    "# For a simple local example, let's create a dummy streaming DataFrame\n",
    "# (In Databricks, you'd typically read from a Delta table or Kafka)\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "import time\n",
    "\n",
    "class MyStreamingListener(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        print(f\"Query started: {event.id}, Name: {event.name}\")\n",
    "    def onQueryProgress(self, event):\n",
    "        print(f\"Query progress: {event.progress}\")\n",
    "    def onQueryTerminated(self, event):\n",
    "        print(f\"Query terminated: {event.id}\")\n",
    "\n",
    "spark.streams.addListener(MyStreamingListener())\n",
    "\n",
    "# Creating a memory source for demonstration\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Create a temporary view to write to from the memory source\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_source AS \n",
    "SELECT CAST(NULL AS STRING) AS item_id, \n",
    "       CAST(NULL AS DOUBLE) AS amount, \n",
    "       CAST(NULL AS TIMESTAMP) AS event_time\n",
    "\"\"\")\n",
    "\n",
    "# Function to inject data into the memory stream (for demonstration)\n",
    "def inject_data(item_id, amount, delay_seconds=0):\n",
    "    current_ts = current_timestamp().cast(TimestampType())\n",
    "    if delay_seconds > 0:\n",
    "        # Simulate late arrival by subtracting delay from current_timestamp\n",
    "        current_ts = (current_timestamp() - expr(f\"INTERVAL {delay_seconds} SECONDS\")).cast(TimestampType())\n",
    "    \n",
    "    data = [(item_id, amount, current_ts)]\n",
    "    df_to_inject = spark.createDataFrame(data, sales_schema)\n",
    "    df_to_inject.write.format(\"delta\").mode(\"append\").saveAsTable(\"bijucatalog.bijusilverschema.sales_stream_input\") # Using a Delta table for simulation\n",
    "\n",
    "    # In a real scenario, you'd be reading from a persistent stream source\n",
    "    # For a purely in-memory simulation, you'd use something like:\n",
    "    # spark.sql(f\"INSERT INTO sales_source VALUES ('{item_id}', {amount}, '{current_ts}')\")\n",
    "\n",
    "# Reading from the simulated Delta table as a stream\n",
    "sales_df = spark.readStream.format(\"delta\").table(\"bijucatalog.bijusilverschema.sales_stream_input\")\n",
    "\n",
    "# Apply watermark and perform windowed aggregation\n",
    "windowed_sales = sales_df \\\n",
    "    .withWatermark(\"event_time\", \"30 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 hour\", \"30 minutes\"), # 1-hour window, sliding every 30 minutes\n",
    "        col(\"item_id\")\n",
    "    ) \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "    .select(\n",
    "        col(\"item_id\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"total_sales\")\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c337bbdb-d41d-43e2-bcac-4803495948e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inject_data(\"keyboard\", 75.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2764eaea-e720-43f5-8880-1564f1ec4d54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bca8c91-ac6c-4702-8071-45a7a650d6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the results to a Delta table in \"append\" mode\n",
    "# For windowed aggregations, \"append\" mode is common, meaning results are emitted once the window is closed by the watermark.\n",
    "query = windowed_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint}/windowed_sales\") \\\n",
    "    .trigger(processingTime=\"1 minute\") \\\n",
    "    .toTable(\"hourly_item_sales\")\n",
    "\n",
    "# Start the query (in Databricks, this runs in the background)\n",
    "query.start() \n",
    "\n",
    "# You can manually inject data to see the effect\n",
    "inject_data(\"laptop\", 1200.0)\n",
    "# time.sleep(5)\n",
    "# inject_data(\"keyboard\", 75.0)\n",
    "# time.sleep(30) # Wait for watermark to potentially advance\n",
    "# inject_data(\"laptop\", 50.0, delay_seconds=15 * 60) # Late data for a past window\n",
    "\n",
    "# To stop the query\n",
    "# query.stop()\n",
    "\n",
    "# To view the results (after the stream has processed some data)\n",
    "# display(spark.sql(\"SELECT * FROM hourly_item_sales ORDER BY window_start DESC\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6191001719175158,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Windowed Aggregation (Calculating Hourly Sales)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
