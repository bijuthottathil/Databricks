{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c48f59-0e90-47bb-a3ef-5646ff00e4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price\", StringType(), True),\n",
    "    StructField(\"order_timestamp\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8add66d6-0ea5-42e3-8ee5-70a5eae9143f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: bronze_orders_load\n",
    "# Language: Python\n",
    "from pyspark.sql.functions import current_timestamp, lit, rand, round, col, input_file_name, to_timestamp, upper\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 1,Configuration\n",
    "raw_data_landing_path = \"abfss://rawdata@adlsexternalfororders.dfs.core.windows.net/orders/\"\n",
    "bronze_table_name = \"ordercatalog.bronze_schema.bronze_orders_raw\"\n",
    "# Checkpoint locations for Structured Streaming \n",
    "bronze_checkpoint_path = f\"{raw_data_path}_checkpoint/bronze_orders/\"\n",
    "silver_checkpoint_path = f\"{raw_data_path}_checkpoint/silver_orders/\"\n",
    "\n",
    "print(f\"Starting Bronze stream to load into {bronze_table_name}...\")\n",
    "\n",
    "bronze_stream_df = spark.readStream \\\n",
    "  .format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"json\") \\\n",
    "  .option(\"cloudFiles.schemaLocation\", f\"{bronze_checkpoint_path}/schema\") \\\n",
    "  .load(raw_data_landing_path) \\\n",
    "  .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "  .withColumn(\"source_file\", input_file_name())\n",
    "\n",
    "# Write the stream to the bronze Delta table in append mode\n",
    "bronze_query = bronze_stream_df.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", f\"{bronze_checkpoint_path}/data\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\\n",
    "  .trigger(availableNow=True).toTable(bronze_table_name)\n",
    "\n",
    "# Wait for the stream to complete the current micro-batch (important for 'availableNow')\n",
    "bronze_query.awaitTermination()\n",
    "print(f\"Bronze stream finished processing. Total records in Bronze: {spark.read.table(bronze_table_name).count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9befa6a9-42b7-4b8d-bcec-c7a597fed44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "silver_table_name = \"ordercatalog.silver_schema.silver_orders_processed\"\n",
    "print(f\"Starting Silver stream to process data from Bronze and upsert into {silver_table_name}...\")\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_to_silver(microBatchOutputDF, batchId):\n",
    "    # Data Cleaning and Transformation for the current micro-batch\n",
    "    df_silver_batch = microBatchOutputDF.select(\n",
    "        col(\"order_id\").cast(StringType()).alias(\"order_id\"),\n",
    "        col(\"customer_id\").cast(StringType()).alias(\"customer_id\"),\n",
    "        col(\"product_id\").cast(StringType()).alias(\"product_id\"),\n",
    "        col(\"quantity\").cast(IntegerType()).alias(\"quantity\"),\n",
    "        col(\"price\").cast(DoubleType()).alias(\"price\"),\n",
    "        # Ensure order_timestamp is correctly cast if it might be string in raw\n",
    "        col(\"order_timestamp\").cast(TimestampType()).alias(\"order_timestamp\"),\n",
    "        upper(col(\"status\")).alias(\"order_status\"), # Standardize status to uppercase\n",
    "        current_timestamp().alias(\"processing_timestamp\") # Add silver layer processing timestamp\n",
    "    )\n",
    "\n",
    "    # Optional: Deduplicate the micro-batch itself before merging\n",
    "    # This prevents issues if a single bronze micro-batch contains multiple updates for the same order_id\n",
    "    df_silver_batch_deduped = df_silver_batch.orderBy(col(\"order_id\"), col(\"order_timestamp\").desc_nulls_last()) \\\n",
    "                                             .dropDuplicates([\"order_id\"])\n",
    "\n",
    "    # Perform MERGE (UPSERT) operation\n",
    "    # The merge key is 'order_id' as it uniquely identifies an order.\n",
    "    merge_key = \"order_id\"\n",
    "\n",
    "    # Check if silver table exists\n",
    "    if not spark.catalog.tableExists(silver_table_name):\n",
    "        # Create the silver table if it doesn't exist (initial run)\n",
    "        df_silver_batch_deduped.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .partitionBy(\"order_status\").saveAsTable(silver_table_name)\n",
    "        print(f\"Batch {batchId}: Created silver table {silver_table_name} and loaded initial data.\")\n",
    "    else:\n",
    "        # If table exists, perform a MERGE\n",
    "        deltaTable = DeltaTable.forName(spark, silver_table_name)\n",
    "\n",
    "        deltaTable.alias(\"target\") \\\n",
    "            .merge(\n",
    "                df_silver_batch_deduped.alias(\"source\"),\n",
    "                f\"target.{merge_key} = source.{merge_key}\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdate(set = { # Update existing records\n",
    "                \"customer_id\": \"source.customer_id\",\n",
    "                \"product_id\": \"source.product_id\",\n",
    "                \"quantity\": \"source.quantity\",\n",
    "                \"price\": \"source.price\",\n",
    "                \"order_timestamp\": \"source.order_timestamp\",\n",
    "                \"order_status\": \"source.order_status\",\n",
    "                \"processing_timestamp\": \"source.processing_timestamp\"\n",
    "            }) \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        print(f\"Batch {batchId}: Performed upsert on silver table {silver_table_name}.\")\n",
    "\n",
    "# Read from bronze as a stream\n",
    "silver_stream_df = spark.readStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .table(bronze_table_name)\n",
    "\n",
    "# Write the stream using foreachBatch for MERGE INTO\n",
    "silver_query = silver_stream_df.writeStream \\\n",
    "  .foreachBatch(upsert_to_silver) \\\n",
    "  .outputMode(\"update\").option(\"checkpointLocation\", f\"{silver_checkpoint_path}/data\").trigger(availableNow=True).start()\n",
    "\n",
    "silver_query.awaitTermination()\n",
    "print(f\"Silver stream finished processing. Total records in Silver: {spark.read.table(silver_table_name).count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 2,4. Gold Layer: Aggregate Results (Batch Processing)\n",
    "# The gold layer aggregates the latest state of data from the silver layer\n",
    "# and is typically refreshed on a schedule (e.g., daily, hourly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62bee57-9a01-4d4a-9536-ca0acc911096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_table_name = \"ordercatalog.gold_schema.gold_orders_daily_summary\"\n",
    "print(f\"Aggregating data for Gold layer from {silver_table_name} into {gold_table_name}...\")\n",
    "\n",
    "from pyspark.sql.functions import sum, count, avg, date_trunc, col, round\n",
    "\n",
    "# Read the current state of the silver table\n",
    "df_silver_current = spark.read.table(silver_table_name)\n",
    "\n",
    "# Perform Aggregations\n",
    "df_gold = df_silver_current.groupBy(\n",
    "    date_trunc(\"day\", col(\"order_timestamp\")).alias(\"order_date\"),\n",
    "    col(\"order_status\")\n",
    ").agg(\n",
    "    count(col(\"order_id\")).alias(\"total_orders\"),\n",
    "    sum(col(\"quantity\")).alias(\"total_quantity\"),\n",
    "    sum(col(\"quantity\") * col(\"price\")).alias(\"total_revenue\"),\n",
    "    round(avg(col(\"price\")), 2).alias(\"average_price_per_item\") # Rounded to 2 decimal places\n",
    ").orderBy(\"order_date\", \"order_status\")\n",
    "\n",
    "# Write to Gold Delta Table (overwrite for daily/periodic summary)\n",
    "# Overwriting ensures idempotency and reflects the latest state from Silver.\n",
    "df_gold.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"overwriteSchema\", \"true\").partitionBy(\"order_date\").saveAsTable(gold_table_name)\n",
    "\n",
    "print(f\"Gold layer aggregation complete. Total records in Gold: {spark.read.table(gold_table_name).count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# DBTITLE 2,5. Verification and Exploration\n",
    "# Verify data in each layer\n",
    "print(\"\\n--- Bronze Layer Sample ---\")\n",
    "spark.sql(f\"SELECT * FROM {bronze_table_name} LIMIT 10\").display()\n",
    "print(f\"Bronze Count: {spark.read.table(bronze_table_name).count()}\")\n",
    "\n",
    "print(\"\\n--- Silver Layer Sample ---\")\n",
    "spark.sql(f\"SELECT * FROM {silver_table_name} LIMIT 10\").display()\n",
    "print(f\"Silver Count: {spark.read.table(silver_table_name).count()}\")\n",
    "\n",
    "print(\"\\n--- Gold Layer Sample ---\")\n",
    "spark.sql(f\"SELECT * FROM {gold_table_name} ORDER BY order_date DESC, order_status LIMIT 10\").display()\n",
    "print(f\"Gold Count: {spark.read.table(gold_table_name).count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5331187493471460,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "EndToEnd_using Autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
